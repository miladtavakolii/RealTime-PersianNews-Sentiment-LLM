from abc import ABC, abstractmethod
from typing import Any
import json
import re
import json


class BaseSentimentProvider(ABC):
    '''
    Abstract base class for sentiment-analysis LLM providers.

    This class defines the shared logic for:
        - Loading and storing prompt templates
        - Building formatted prompts for a given article
        - Extracting and validating JSON output from LLM responses

    Subclasses (e.g., Ollama, Gemini, OpenAI) must implement the `generate` method
    responsible for sending prompts to their specific model backends.

    Attributes:
        prompt_template:
            The raw prompt template loaded from configuration or file.
            It may contain placeholders to be filled using `build_prompt`.
    '''

    def __init__(self, prompt_template: str):
        '''
        Initialize the provider with a prompt template.

        Args:
            prompt_template:
                The base prompt string containing placeholders for fields
                such as title, summary, categories, etc.
        '''
        self.prompt_template = prompt_template

    @abstractmethod
    def generate(self, prompt: str) -> str:
        '''
        Send the input prompt to the model and return the raw (unprocessed)
        string output from the LLM.

        Args:
            prompt: Fully-rendered prompt prepared for the LLM.

        Returns:
            The raw output text generated by the model.
        '''
        pass

    def build_prompt(self, **kwargs: Any) -> str:
        '''
        Construct the final prompt by filling template placeholders.

        Args:
            **kwargs:
                Arbitrary keyword arguments representing article fields.
                Example fields: `title`, `summary`, `content`, etc.

        Returns:
            The formatted prompt ready to be passed to the LLM.
        '''
        return self.prompt_template.format(**kwargs)

    def extract_json(self, text: str) -> dict:
        '''
        Extract and parse a valid JSON object from arbitrary LLM output.

        This method:
            - Removes markdown code fences (```json ... ```)
            - Normalizes non-standard quotes
            - Attempts full-text JSON parsing
            - Falls back to regex-based JSON extraction when needed

        Args:
            text: Raw output returned by the LLM.

        Returns:
            Parsed JSON object extracted from the model output.

        Raises:
            ValueError:
                If no valid JSON structure can be recovered.
        '''
        text = text.replace('â€', '\'')
        # Remove markdown code fences
        text = text.strip()
        text = re.sub(r"^```[a-zA-Z]*", "", text)
        text = re.sub(r"```$", "", text)
        text = text.strip()

        # Try parse as JSON directly
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # If JSON is inside text somewhere:
        match = re.search(r"\{[\s\S]*\}", text)
        if match:
            return json.loads(match.group(0))

        raise ValueError('[ERROR] No valid JSON found in model output')

    def analyze(self, **fields: Any) -> dict:
        '''
        High-level workflow for performing sentiment analysis with an LLM provider.

        Steps:
            1. Construct prompt using article fields
            2. Generate raw model output via `generate`
            3. Extract and validate JSON sentiment result

        Args:
            **fields:
                Article metadata and text content required to fill the prompt.

        Returns:
            Structured sentiment analysis result.

        Raises:
            ValueError:
                If the model returns invalid or unparsable JSON.
        '''
        prompt = self.build_prompt(**fields)
        raw_output = self.generate(prompt)

        try:
            return self.extract_json(raw_output)
        except json.JSONDecodeError:
            raise ValueError(
                f'[ERROR] Provider returned invalid JSON:\n{raw_output}')
